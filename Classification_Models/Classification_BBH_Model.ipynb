{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"zdFPCv7GY_bV"},"outputs":[],"source":["# !unzip -q '/content/gdrive/MyDrive/Positive_Training_DATA_O.zip' -d '/content/gdrive/MyDrive/Positive_Training_DATA'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wncwmnAlpcWj"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/gdrive',force_remount=True)\n","!ls -lt '/content/gdrive/My Drive/' \n","import numpy as np\n","import tensorflow as tf\n","from tensorflow.keras.layers import Dense, Flatten, MaxPooling1D\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Conv1D, Dropout\n","from tensorflow.keras.optimizers import Adam, RMSprop\n","from tensorflow.keras.losses import BinaryCrossentropy\n","from tensorflow.keras.metrics import binary_accuracy, AUC\n","from sklearn.utils import shuffle"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yFopl-9x54AD"},"outputs":[],"source":["def model_creation():\n","  n_timesteps, n_features = 16384, 1\n","  input_shape=(n_timesteps,n_features)\n","  model=Sequential()\n","\n","  model.add(Conv1D(filters=32, kernel_size=16, activation='relu', input_shape=input_shape))\n","  model.add(MaxPooling1D(pool_size=4))\n","\n","  model.add(Conv1D(64, kernel_size=8, activation='relu'))\n","  model.add(MaxPooling1D(pool_size=4))\n","\n","  model.add(Conv1D(128, kernel_size=8, activation='relu'))\n","  #model.add(Conv1D(16, kernel_size=16, activation='relu'))\n","  model.add(MaxPooling1D(pool_size=4))\n","\n","  model.add(Conv1D(256, kernel_size=8, activation='relu'))\n","  #model.add(Conv1D(32, kernel_size=16, activation='relu'))\n","  model.add(MaxPooling1D(pool_size=4))\n","\n","  model.add(Flatten())\n","\n","  model.add(Dense(128, activation='relu'))\n","  model.add(Dropout(0.5))\n","\n","  model.add(Dense(64, activation='relu'))\n","  model.add(Dropout(0.5))\n","\n","  model.add(Dense(1, activation='sigmoid'))\n","\n","  model.compile(optimizer= Adam(learning_rate=0.002, beta_1=0.9, beta_2=0.999, epsilon=1e-08),\n","                loss='binary_crossentropy',\n","                metrics=['binary_accuracy', 'accuracy'])\n","  model.summary()\n","  return model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9iI3yuusbKxe","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1641884877238,"user_tz":-330,"elapsed":398,"user":{"displayName":"MITHIL JOSHI","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10006108984093216155"}},"outputId":"a3168ecf-907d-4863-80a9-53b73160b836"},"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv1d (Conv1D)             (None, 16369, 32)         544       \n","                                                                 \n"," max_pooling1d (MaxPooling1D  (None, 4092, 32)         0         \n"," )                                                               \n","                                                                 \n"," conv1d_1 (Conv1D)           (None, 4085, 64)          16448     \n","                                                                 \n"," max_pooling1d_1 (MaxPooling  (None, 1021, 64)         0         \n"," 1D)                                                             \n","                                                                 \n"," conv1d_2 (Conv1D)           (None, 1014, 128)         65664     \n","                                                                 \n"," max_pooling1d_2 (MaxPooling  (None, 253, 128)         0         \n"," 1D)                                                             \n","                                                                 \n"," conv1d_3 (Conv1D)           (None, 246, 256)          262400    \n","                                                                 \n"," max_pooling1d_3 (MaxPooling  (None, 61, 256)          0         \n"," 1D)                                                             \n","                                                                 \n"," flatten (Flatten)           (None, 15616)             0         \n","                                                                 \n"," dense (Dense)               (None, 128)               1998976   \n","                                                                 \n"," dropout (Dropout)           (None, 128)               0         \n","                                                                 \n"," dense_1 (Dense)             (None, 64)                8256      \n","                                                                 \n"," dropout_1 (Dropout)         (None, 64)                0         \n","                                                                 \n"," dense_2 (Dense)             (None, 1)                 65        \n","                                                                 \n","=================================================================\n","Total params: 2,352,353\n","Trainable params: 2,352,353\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}],"source":["from keras.models import load_model\n","model = model_creation()\n","# model.save(\"/content/gdrive/MyDrive/BBH_Classification_Model.h5\")\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_Skv64TyPWnF","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1641884841778,"user_tz":-330,"elapsed":700,"user":{"displayName":"MITHIL JOSHI","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10006108984093216155"}},"outputId":"74d483df-2d49-42e8-e8c4-26e2ee3dc8c8"},"outputs":[{"output_type":"stream","name":"stdout","text":["512\n","(512, 1)\n"]}],"source":["# training file locations\n","postrainfoldername = '/content/gdrive/MyDrive/Positive_Training_DATA'\n","negtrainfoldername = '/content/gdrive/MyDrive/Negative_Train_DATA'\n","\n","# postive and negative prefixes\n","postrainprefix = 111100000\n","negtrainprefix = 101100000\n","\n","#samples of pos and neg training data\n","postrainsamples = 24576\n","negtrainsamples = 8192\n","\n","# initialises global variables \n","def initialise():\n","  global postrainprefix\n","  global negtrainprefix\n","  postrainprefix = 111100000\n","  negtrainprefix = 101100000\n","\n","# number of batches\n","batch_looping_times = 64\n","\n","# number of samples in each batch\n","samples = (postrainsamples//batch_looping_times) +(negtrainsamples//batch_looping_times)\n","\n","print(samples)\n","y=np.concatenate((np.ones(((postrainsamples//batch_looping_times), 1)), np.zeros(((negtrainsamples//batch_looping_times), 1))))\n","print(y.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"U8CUPqNmQG0C"},"outputs":[],"source":["def batchload_train():\n","  global y\n","  global postrainprefix\n","  global negtrainprefix\n","  # print(samples)\n","  X=np.zeros((samples,8192*2))\n","  # y=np.zeros((samples,1))\n","  for i in range(postrainsamples//batch_looping_times):\n","    posfile_name = postrainfoldername+'/'+str(postrainprefix+i+1)+'.npy'\n","    X[i] = np.load(posfile_name)\n","    # y[i] = 1\n","  for i in range(negtrainsamples//batch_looping_times):\n","    negfile_name = negtrainfoldername+'/'+str(negtrainprefix+i+1)+'.npy'\n","    X[i] = np.load(negfile_name)\n","    # y[i] = 0\n","\n","  X = X.reshape(samples, 16384,1)\n","  \n","  return X, y\n","\n","# trainX, trainy = batchload_train()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"A1HrpxNn2j3x"},"outputs":[],"source":["def get_data():\n","  global postrainprefix\n","  global negtrainprefix\n","  global y\n","  initialise()\n","  while True:\n","\n","    trainX, trainy = batchload_train()\n","    # train_dataset = tf.data.Dataset.from_tensor_slices((trainX, trainy)).shuffle(729)\n","    # train_dataset = train_dataset.batch(batch_size)\n","    trainX, trainy = shuffle(trainX, trainy, random_state=53)\n","    postrainprefix += (postrainsamples//batch_looping_times)\n","    negtrainprefix += (negtrainsamples//batch_looping_times)\n","    if (postrainprefix >= 111124575):\n","      initialise()\n","    yield trainX, trainy\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wb28tH_u8lXB"},"outputs":[],"source":["from tensorflow.keras.callbacks import ModelCheckpoint\n","checkpoint_path = \"/content/gdrive/My Drive/BBH_Classification.ckpt\"\n","\n","\n","# Create a callback that saves the model's weights every 5 epochs\n","cp_callback = ModelCheckpoint(\n","    filepath=checkpoint_path, \n","    verbose=0, \n","    save_weights_only=True,\n","    save_freq = 1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9QF0e7iAYjt9"},"outputs":[],"source":["from keras.models import load_model\n","def model_training():\n","\n","    # model = load_model(\"/content/gdrive/MyDrive/BBH_Classification_Model.h5\")\n","    model.fit(get_data(), epochs=8, steps_per_epoch=64, verbose = 1, shuffle=\n","                        True, callbacks=[cp_callback], batch_size = 512, use_multiprocessing = True)\n","    model.save(\"/content/gdrive/MyDrive/BBH_Classification_Model_1.h5\")\n","\n","    \n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"b-tsYnO7gKz3","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1641897876756,"user_tz":-330,"elapsed":12984265,"user":{"displayName":"MITHIL JOSHI","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10006108984093216155"}},"outputId":"8285144a-53aa-43c7-b674-fc48c5d10cfe"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/8\n","64/64 [==============================] - 4714s 74s/step - loss: 0.0990 - binary_accuracy: 0.9718 - accuracy: 0.9718\n","Epoch 2/8\n","64/64 [==============================] - 1164s 18s/step - loss: 1.2011e-04 - binary_accuracy: 1.0000 - accuracy: 1.0000\n","Epoch 3/8\n","64/64 [==============================] - 1175s 18s/step - loss: 2.1940e-04 - binary_accuracy: 0.9999 - accuracy: 0.9999\n","Epoch 4/8\n","64/64 [==============================] - 1182s 18s/step - loss: 7.2885e-05 - binary_accuracy: 1.0000 - accuracy: 1.0000\n","Epoch 5/8\n","64/64 [==============================] - 1181s 18s/step - loss: 0.3680 - binary_accuracy: 0.9999 - accuracy: 0.9999\n","Epoch 6/8\n","64/64 [==============================] - 1182s 18s/step - loss: 4.4011e-05 - binary_accuracy: 1.0000 - accuracy: 1.0000\n","Epoch 7/8\n","64/64 [==============================] - 1185s 19s/step - loss: 4.5830e-04 - binary_accuracy: 0.9999 - accuracy: 0.9999\n","Epoch 8/8\n","64/64 [==============================] - 1181s 18s/step - loss: 3.1334e-04 - binary_accuracy: 0.9999 - accuracy: 0.9999\n"]}],"source":["model_training()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YQmf05qupcH1"},"outputs":[],"source":["# def evaluation_model(trainX, trainy, verbose, epochs, batch_size):\n","#   train_dataset = tf.data.Dataset.from_tensor_slices((trainX, trainy)).shuffle(35000)\n","#   train_dataset = train_dataset.batch(batch_size)\n","#   model = model_creation(trainX)\n","#   model.fit(train_dataset, epochs=epochs, batch_size=batch_size, verbose=verbose)\n","\n","#   test_dataset = tf.data.Dataset.from_tensor_slices((testX, testy)).shuffle(35000)\n","#   # test_dataset = test_dataset.batch(batch_size)\n","#   _, accuracy = model.evaluate(test_dataset, batch_size=batch_size, verbose=verbose)\n","#   return accuracy\n","\n","# evaluation_model(X, y, 0, 1, 1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xG4AY6M5pcNz"},"outputs":[],"source":["# def test_data_load(posfoldername, negfoldername,  posprefix, negprefix):\n","#   testX = list()\n","#   testy = list()\n","#   for i in range(4):\n","#     posfile_name = posfoldername+'/'+str(posprefix+i+1)+'.npy'\n","#     testX.append(np.load(posfile_name))\n","#     testy.append(1)\n","\n","\n","#   for i in range(2):\n","#     negfile_name = negfoldername+'/'+str(negprefix+i+1)+'.npy'\n","#     testX.append(np.load(negfile_name))\n","#     testy.append(0)\n","\n","#   testy = tf.stack(np.array(testy))\n","#   testX = tf.stack(np.array(testX).reshape(len(testy),16384,1))\n","\n","#   print(testX.shape)\n","#   print(testy.shape)\n","\n","#   return testX, testy\n","\n","# testX, testy = test_data_load('/content/gdrive/MyDrive/Positive_Test_DATA', '/content/gdrive/MyDrive/Negative_Test_DATA', 111000000, 101000000)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iX8YIr0hpcQo"},"outputs":[],"source":["# def validation_data_load(posfoldername, negfoldername,  posprefix, negprefix):\n","#   validX = list()\n","#   validy = list()\n","#   for i in range(4):\n","#     posfile_name = posfoldername+'/'+str(posprefix+i+1)+'.npy'\n","#     validX.append(np.load(posfile_name))\n","#     validy.append(1)\n","\n","\n","#   for i in range(2):\n","#     negfile_name = negfoldername+'/'+str(negprefix+i+1)+'.npy'\n","#     validX.append(np.load(negfile_name))\n","#     validy.append(0)\n","\n","#   validy = tf.stack(np.array(validy))\n","#   validX = tf.stack(np.array(validX).reshape(len(validy),16384,1))\n","\n","#   print(validX.shape)\n","#   print(validy.shape)\n","\n","#   return validX, validy"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WWJqCQD_pcE9"},"outputs":[],"source":["# # summarize scores\n","# def summarize_results(scores):\n","# \tprint(scores)\n","# \tm, s = mean(scores), std(scores)\n","# \tprint('Accuracy: %.3f%% (+/-%.3f)' % (m, s))\n","\n","# # run an experiment\n","# def run_experiment(repeats=10):\n","# \t# load data\n","# \ttrainX, trainy, testX, testy = load_dataset()\n","# \t# repeat experiment\n","# \tscores = list()\n","# \tfor r in range(repeats):\n","# \t\tscore = evaluate_model(trainX, trainy, testX, testy)\n","# \t\tscore = score * 100.0\n","# \t\tprint('>#%d: %.3f' % (r+1, score))\n","# \t\tscores.append(score)\n","# \t# summarize results\n","# \tsummarize_results(scores)\n","\n","# # run the experiment\n","# run_experiment()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"S9V4lOb4tl0M","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1641883650447,"user_tz":-330,"elapsed":538,"user":{"displayName":"MITHIL JOSHI","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10006108984093216155"}},"outputId":"9462a409-e592-490e-f0d0-e46e4652b93a"},"outputs":[{"output_type":"stream","name":"stdout","text":["(32, 16384)\n","(512, 1)\n","(512, 1)\n","[0. 0. 0. ... 0. 0. 0.]\n","[0.]\n"]}],"source":["# import numpy as np\n","\n","# X=np.zeros((32,8192*2))\n","# print(X.shape)\n","# y=np.zeros((512,1))\n","# print(y.shape)\n","\n","\n","# Y=np.concatenate((np.ones((384, 1)), np.zeros((128, 1))))\n","# print(Y.shape)\n","# posprefix = 111100000\n","# posfoldername = '/content/gdrive/MyDrive/Positive_Training_DATA'\n","\n","# posfile_name = posfoldername+'/'+str(posprefix+1+24575)+'.npy'\n","# print(X[0])\n","# print(y[0])\n","# X[0] = np.load(posfile_name)\n","# y[0] = 1\n","# print(X[0])\n","# print(y[0])\n","# X = X.reshape(32, 16384,1)\n","# # y = y.reshape(32, 1, 1)\n","# print(X[0])\n","# print(y[0])\n","\n","# print(y)\n","\n","# print(X.shape)\n","# print(y.shape)\n","\n","# # make sure all the samples are there in respective files\n","# # import data in batches of 32/64/128\n","# # train the model on this data\n","# # save the model weights\n","# # update prefixes\n","# # import next batch\n","# # # repeat above for all batches of data\n","# # repeat above for 10 times i.e. 10 epochs"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uSWqD2NUpcTo"},"outputs":[],"source":["# import numpy as np\n","# def training_data_load(posfoldername, negfoldername,  posprefix, negprefix, valposfoldername, valnegfoldername,  valposprefix, valnegprefix):\n","#   trainX = list()\n","#   trainy = list()\n","#   for i in range(2):\n","#     posfile_name = posfoldername+'/'+str(posprefix+i+1)+'.npy'\n","#     trainX.append(np.load(posfile_name))\n","#     trainy.append(1)\n","  \n","#   # for i in range(3072):\n","#   #   valposfile_name = valposfoldername+'/'+str(valposprefix+i+1)+'.npy'\n","#   #   trainX.append(np.load(valposfile_name))\n","#   #   trainy.append(1)\n","\n","\n","#   for i in range(2):\n","#     negfile_name = negfoldername+'/'+str(negprefix+i+1)+'.npy'\n","#     trainX.append(np.load(negfile_name))\n","#     trainy.append(0)\n","\n","#   # for i in range(1024):\n","#   #   valnegfile_name = valnegfoldername+'/'+str(valnegprefix+i+1)+'.npy'\n","#   #   trainX.append(np.load(valnegfile_name))\n","#   #   trainy.append(0)\n","\n","#   trainy = tf.stack(np.array(trainy))\n","#   trainX = tf.stack(np.array(trainX).reshape(len(trainy),16384,1))\n","\n","#   print(trainX.shape)\n","#   print(trainy.shape)\n","\n","#   return trainX, trainy\n","# trainX, trainy = training_data_load('/content/gdrive/MyDrive/Positive_Training_DATA', '/content/gdrive/MyDrive/Negative_Train_DATA', 111100000, 101100000\n","#                           ,'/content/gdrive/MyDrive/Positive_Validation_DATA', '/content/gdrive/MyDrive/Negative_Validation_DATA', 110100000, 100100000)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NSa1IkG3I6Uw"},"outputs":[],"source":["# prefix = 111100000\n","\n","# def ini():\n","#   global prefix\n","#   prefix = 111100000\n","\n","# while True:\n","#   global prefix\n","#   print(prefix)\n","#   prefix +=1\n","#   if prefix>=111100010:\n","#     ini()"]},{"cell_type":"code","source":["# import sys\n","# !{sys.executable} -m pip install pycbc ligo-common --no-cache-dir"],"metadata":{"id":"H9boP5xlG1lz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# import numpy as np\n","# import math\n","# import pylab\n","\n","# import matplotlib.pyplot as plt\n","# import random\n","# import pycbc\n","# from pycbc import distributions\n","# from pycbc.waveform import get_td_waveform\n","# from pycbc.detector import Detector\n","# import pycbc.coordinates as co\n","# from pycbc.psd import welch, interpolate\n","# from pycbc.psd import interpolate, inverse_spectrum_truncation\n","# from pycbc.noise.gaussian import noise_from_psd\n","# from pycbc.noise.gaussian import frequency_noise_from_psd\n","# from pycbc.filter import matched_filter\n","\n","# det_l1 = Detector('L1')\n","# apx = 'IMRPhenomD'\n","# N=2048*16  #N is number of samples, N=length/delta_t\n","# fs=2048 #fs is sampling frequnecy\n","# length=16 #duration of segment\n","# delta_f=1.0/16\n","# f_samples = 16385\n","# f_lower=30\n","# delta_t=1.0/2048\n","\n","# from pycbc.psd.analytical import AdVDesignSensitivityP1200087\n","\n","# def get_psd(f_samples, delta_f, low_freq_cutoff):\n","#     psd=AdVDesignSensitivityP1200087(f_samples, delta_f, low_freq_cutoff)\n","#     return psd\n","\n","# from pycbc.noise.gaussian import frequency_noise_from_psd\n","\n","# def get_noise(psd, seed=None):\n","#     noise=frequency_noise_from_psd(psd, seed=seed)\n","#     noise_time = noise.to_timeseries()\n","#     return noise_time\n","\n","# def add_noise_signal(noise, signal):\n","#     length_signal = len(signal)\n","#     signal_plus_noise=noise\n","#     signal_plus_noise[0:length_signal]=np.add(noise[0:length_signal], signal)\n","#     return signal_plus_noise\n","\n","\n","# from pycbc.psd import welch, interpolate\n","\n","# def get_whiten(signal_plus_noise):\n","#     signal_freq_series=signal_plus_noise.to_frequencyseries()\n","#     numerator = signal_freq_series\n","#     psd_to_whiten = interpolate(welch(signal_plus_noise), 1.0 / signal_plus_noise.duration)\n","#     denominator=np.sqrt(psd_to_whiten)\n","#     whiten_freq = (numerator / denominator)\n","#     whiten=whiten_freq.to_timeseries().highpass_fir(30., 512).lowpass_fir(300.0, 512)\n","#     return whiten\n","\n","# def get_8s(whiten, signal_peak_index=None):\n","#     whiten.start_time = 0\n","#     cropped = whiten.time_slice(0,8)\n","#     return cropped\n","\n","# psd=get_psd(f_samples, delta_f, f_lower)\n","\n","# def DISTRIBUTIONS(low, high, samples):\n","#     var_dist = distributions.Uniform(var = (low, high))\n","#     return var_dist.rvs(size = samples)\n","\n","# def SPIN_DISTRIBUTIONS(samples):\n","#     theta_low = 0.\n","#     theta_high = 1.\n","#     phi_low = 0.\n","#     phi_high = 2.\n","#     uniform_solid_angle_distribution = distributions.UniformSolidAngle(polar_bounds=(theta_low,theta_high),\n","#                                               azimuthal_bounds=(phi_low,phi_high))\n","#     solid_angle_samples = uniform_solid_angle_distribution.rvs(size=samples)\n","#     spin_mag = np.ndarray(shape=(samples), dtype=float)\n","#     for i in range(0,samples):\n","#         spin_mag[i] = 1.\n","#     spinx, spiny, spinz = co.spherical_to_cartesian(spin_mag,solid_angle_samples['phi'],solid_angle_samples['theta'])\n","#     return spinz\n","\n","# def get_params(samples):\n","#     mass1_samples = DISTRIBUTIONS(10, 80, samples)\n","#     mass2_samples = DISTRIBUTIONS(10, 80, samples)\n","#     right_ascension_samples  = DISTRIBUTIONS(0 , 2*math.pi, samples)\n","#     polarization_samples = DISTRIBUTIONS(0 , 2*math.pi, samples)\n","#     declination_samples = DISTRIBUTIONS((-math.pi/2)+0.0001, (math.pi/2)-0.0001, samples)\n","#     spinz1 = SPIN_DISTRIBUTIONS(samples)\n","#     spinz2 = SPIN_DISTRIBUTIONS(samples)\n","#     snr_req = DISTRIBUTIONS(2, 17, samples)\n","#     DIST = DISTRIBUTIONS(2500, 3000, samples)\n","#     return mass1_samples, mass2_samples, right_ascension_samples, polarization_samples, declination_samples, spinz1, spinz2, snr_req, DIST\n","\n","# def DATA_GENERATION(samples):\n","\n","#   mass1_samples, mass2_samples, right_ascension_samples, polarization_samples, declination_samples, spinz1, spinz2, snr_req, DIST = get_params(samples)\n","#   for i in range(0,samples):\n","#         seed =  random.randint(1, 256)\n","#         # NOTE: Inclination runs from 0 to pi, with poles at 0 and pi\n","#         #       coa_phase runs from 0 to 2 pi.\n","#         try:\n","#           hp, hc = get_td_waveform(approximant=apx,\n","#                                   mass1=mass1_samples[i][0],\n","#                                   mass2=mass2_samples[i][0],\n","#                                   spin1z=spinz1[i],\n","#                                   spin2z=spinz2[i],\n","#                                   delta_t=delta_t,\n","#                                   distance = DIST[i][0],\n","#                                   f_lower=40)\n","#         except:\n","#           try:\n","#             hp, hc = get_td_waveform(approximant=apx,\n","#                           mass1=mass1_samples[i][0],\n","#                           mass2=mass2_samples[i][0],\n","#                           spin1z=spinz1[i],\n","#                           spin2z=spinz2[i],\n","#                           delta_t=delta_t,\n","#                           distance = DIST[i][0],\n","#                           f_lower=50)\n","#           except RuntimeError:\n","#             hp, hc = get_td_waveform(approximant=apx,\n","#                                   mass1=mass1_samples[i][0],\n","#                                   mass2=mass2_samples[i][0],\n","#                                   spin1z=spinz1[i],\n","#                                   spin2z=spinz2[i],\n","#                                   delta_t=delta_t*2,\n","#                                   distance = DIST[i][0],\n","#                                   f_lower=40)\n","             \n","\n","#         signal_l1 = det_l1.project_wave(hp, hc,  right_ascension_samples[i][0], declination_samples[i][0], polarization_samples[i][0])\n","#         signal_l1.append_zeros(10*2048)\n","#         signal_l1 = signal_l1.cyclic_time_shift(5)\n","#         signal_l1.start_time = 0 \n","\n","#         noise=get_noise(psd)\n","#         final = add_noise_signal(noise, signal_l1)\n","\n","#         hps=signal_l1\n","#         conditioned=final\n","#         hps.resize(len(conditioned))\n","#         template = hps.cyclic_time_shift(hps.start_time)\n","#         psd_whiten=interpolate(welch(conditioned), 1.0 / conditioned.duration)\n","#         snr = matched_filter(template, conditioned, psd=psd_whiten, low_frequency_cutoff=40, sigmasq = 1)\n","#         peak = abs(snr).numpy().argmax()\n","#         snrp = snr[peak]\n","#         time = snr.sample_times[peak]\n","\n","#         signal_l1_scaled = signal_l1*snr_req[i][0] / abs(snrp) \n","\n","\n","#         final_scaled = add_noise_signal(noise, signal_l1_scaled)\n","\n","\n","#         whiten = get_whiten (final_scaled)\n","\n","\n","#         data = get_8s(whiten)\n","\n","#         my_dir = '/content/gdrive/MyDrive/Positive_Validation_DATA/'# write the file name in which you need to put the data\n","#         name = 110100000+i+1+2232\n","#         np.save(my_dir + str(name), data)\n"],"metadata":{"id":"CliaCErBHKl0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","# DATA_GENERATION(1)"],"metadata":{"id":"h2-p-hkwHWm2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"OUpq-ViDHsnU"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"collapsed_sections":[],"name":"Classification_BBH_Model.ipynb","provenance":[],"authorship_tag":"ABX9TyOSzV/ZXptn6VQOLoWRhtZh"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}